{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection through Image Scraping\n",
    "\n",
    "In this notebook, we will explore two different ways to scrape images from the web:\n",
    "\n",
    "**[BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#quick-start)** - Best for static HTML pages\n",
    "\n",
    "**[Gallery-DL](https://github.com/mikf/gallery-dl)** - Best for structured bulk downloading from image-heavy sites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you proceed with the code examples, make sure you install the necessairy modules.\n",
    "\n",
    "Beware, there might be conflicts with some dependencies, so if something doesn't get installed or work properly for you, take it easy. You can work around it or choose another method to scrape images from the web. You are given many options here.\n",
    "\n",
    "Feel free to also explore **[Selenium](https://selenium-python.readthedocs.io/getting-started.html)**, which is best for JavaScript-heavy, dynamically loaded content.\n",
    "\n",
    "An extra, code free option for creating a dataset, is to use **[Kaggle](https://www.kaggle.com)**. Kaggle holds many public datasets e.g. [animal image dataset](https://www.kaggle.com/datasets/iamsouravbanerjee/animal-image-dataset-90-different-animals/data) that you can download.\n",
    "\n",
    "**NEVER FORGET TO DECLARE WHERE YOU EXTRACTED YOUR DATA FROM, NO MATTER WHICH OF THESE APPROACHES YOU CHOOSE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install -c conda-forge beautifulsoup4 selenium webdriver-manager\n",
    "# !pip install gallery_dl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beautiful Soup | a simple HTML scraper\n",
    "\n",
    "It is best for basic scraping from static HTML pages, like [wikipedia](https://www.wikipedia.org) and [bbc](https://www.bbc.co.uk)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping text from a website\n",
    "# This script will scrape the headlines from the BBC News website\n",
    "url = \"https://www.bbc.co.uk/news\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Find all headlines\n",
    "headlines = soup.find_all(\"h3\")\n",
    "\n",
    "# Print the headlines\n",
    "for headline in headlines:\n",
    "    print(headline.text.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define base directory\n",
    "bs_dataset_txt = \"./data/bs_dataset_txt\"\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(bs_dataset_txt, exist_ok=True)\n",
    "\n",
    "# Find all headlines\n",
    "headlines = [h3.text.strip() for h3 in soup.find_all(\"h3\")]\n",
    "\n",
    "# Save to a text file inside `bs_dataset/`\n",
    "file_path = os.path.join(bs_dataset_txt, \"headlines.txt\")\n",
    "with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for headline in headlines:\n",
    "        f.write(headline + \"\\n\")\n",
    "\n",
    "print(f\"Saved {len(headlines)} headlines to {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly to scraping text, Beautiful Soup can be used to scrape images. \n",
    "\n",
    "Note that this method does not work on websites where images are loaded dynamically using JavaScript."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping images from a website\n",
    "# This script will scrape the image URLs from the BBC News website\n",
    "url = \"https://www.bbc.co.uk/news\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Find all image tags\n",
    "images = soup.find_all(\"img\")\n",
    "\n",
    "# Print image URLs\n",
    "for img in images:\n",
    "    print(img[\"src\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define base directory\n",
    "bs_dataset_imgs = \"./data/bs_dataset_imgs\"\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(bs_dataset_imgs, exist_ok=True)\n",
    "\n",
    "# Download and save images\n",
    "for idx, img in enumerate(images[:10]):  # Limit to first 10 images\n",
    "    img_url = img.get(\"src\")\n",
    "\n",
    "    if img_url and img_url.startswith(\"http\"):  # Ensure it's a valid URL\n",
    "        img_path = os.path.join(bs_dataset_imgs, f\"image_{idx}.jpg\")\n",
    "\n",
    "        # Download and save the image\n",
    "        img_data = requests.get(img_url).content\n",
    "        with open(img_path, \"wb\") as f:\n",
    "            f.write(img_data)\n",
    "\n",
    "print(f\"Downloaded {len(images[:10])} images into {bs_dataset_imgs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gallery-DL | a bulk image scraper\n",
    "\n",
    "It is best for structured bulk downloads, from [Pinterest](https://uk.pinterest.com), [Instagram](https://www.instagram.com), [Flickr](https://www.flickr.com), and similar sites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the package is installed\n",
    "!gallery-dl --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will organize the images into folders like class_1, class_2, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import json\n",
    "\n",
    "# Define base dataset path\n",
    "my_dataset_path = \"./data/gallery_dl_dataset\"\n",
    "\n",
    "# Number of classes you need (change if needed)\n",
    "num_classes = 3  \n",
    "\n",
    "# Create base dataset directory if it doesn't exist\n",
    "os.makedirs(my_dataset_path, exist_ok=True)\n",
    "\n",
    "# Create multiple class folders dynamically\n",
    "class_folders = []\n",
    "for i in range(1, num_classes + 1):\n",
    "    class_folder = os.path.join(my_dataset_path, f\"class_{i}\")\n",
    "    os.makedirs(class_folder, exist_ok=True)\n",
    "    class_folders.append(class_folder)\n",
    "\n",
    "print(f\"Created class folders: {class_folders}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We modify Gallery-DL’s settings to download only image files and exclude unnecessary files like JSON metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define gallery-dl config directory **** choose one of the following based on your OS\n",
    "config_dir = os.path.expanduser(\"~/.config/gallery-dl\")  # for Linux/macOS\n",
    "config_path = os.path.join(config_dir, \"config.json\")\n",
    "# config_dir = os.path.expanduser(\"~\\\\AppData\\\\Local\\\\gallery-dl\") # for Windows\n",
    "\n",
    "# Ensure the config directory exists\n",
    "os.makedirs(config_dir, exist_ok=True)\n",
    "\n",
    "# Define the config to exclude non-image files\n",
    "gallery_dl_config = {\n",
    "    \"extractor\": {\n",
    "        \"base-directory\": \"pinterest_downloads\",  # Base directory\n",
    "        \"directory\": [\"class-{num}\"],  # Organizes into class folders\n",
    "        \"skip-metadata\": True,  # Prevents JSON files from being downloaded\n",
    "        \"archive\": False,  # Avoids unnecessary archive files\n",
    "        \"postprocessors\": [],\n",
    "        \"filter\": \"extension in ('jpg', 'jpeg', 'png', 'gif', 'webp')\"  # Filters only image files\n",
    "    }\n",
    "}\n",
    "\n",
    "# Write to the config file\n",
    "with open(config_path, \"w\") as f:\n",
    "    json.dump(gallery_dl_config, f, indent=4)\n",
    "\n",
    "print(f\"Gallery-DL configuration updated at: {config_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will store images from each Pinterest board into separate folders (class_1, class_2, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Pinterest boards to download from\n",
    "pinterest_boards = [\n",
    "    \"https://uk.pinterest.com/dfordesignoc/_-mid-century-modern/eichler-homes/\",\n",
    "    \"https://uk.pinterest.com/dfordesignoc/_-mid-century-modern/eichler-homes/\",\n",
    "    \"https://uk.pinterest.com/dfordesignoc/_-mid-century-modern/eichler-homes/\",\n",
    "]\n",
    "\n",
    "# Download images from each board\n",
    "for idx, board_url in enumerate(pinterest_boards):\n",
    "    class_folder = class_folders[idx % len(class_folders)]  # Rotate through class folders\n",
    "    print(f\"Downloading from {board_url} into {class_folder}...\")\n",
    "\n",
    "    # Run gallery-dl (Downloads images from Pinterest board)\n",
    "    !gallery-dl -d \"{class_folder}\" \"{board_url}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gallery-DL may create extra subdirectories, so we need to move images directly into the class folder. \n",
    "\n",
    "This flattens the directory structure so all images are directly inside class_X folders. \n",
    "\n",
    "If this confuses you or fails for some reason, you can always organise your folders manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to move images from subdirectories to the main class folder\n",
    "def move_images_to_class_folder(class_folder):\n",
    "    \"\"\"\n",
    "    Moves all images from subdirectories inside `class_folder`\n",
    "    directly into `class_folder`, flattening the structure.\n",
    "    \"\"\"\n",
    "    for root, dirs, files in os.walk(class_folder, topdown=False):\n",
    "        for file_name in files:\n",
    "            file_path = os.path.join(root, file_name)\n",
    "            new_path = os.path.join(class_folder, file_name)\n",
    "\n",
    "            # Move the file only if it's not already in the main class folder\n",
    "            if root != class_folder:\n",
    "                shutil.move(file_path, new_path)\n",
    "                print(f\"Moved {file_path} → {new_path}\")\n",
    "\n",
    "        # Remove empty subdirectories after moving files\n",
    "        for dir_name in dirs:\n",
    "            dir_path = os.path.join(root, dir_name)\n",
    "            if not os.listdir(dir_path):  # Check if empty\n",
    "                os.rmdir(dir_path)\n",
    "                print(f\"Removed empty directory: {dir_path}\")\n",
    "\n",
    "# Apply function to all class folders\n",
    "for class_folder in class_folders:\n",
    "    move_images_to_class_folder(class_folder)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dmlap25my",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
